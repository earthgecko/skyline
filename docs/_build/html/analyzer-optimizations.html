

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Analyzer Optimizations &mdash; Skyline 1.0.6-dev documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="_static/skyline.styles.css" type="text/css" />
  

  
    <link rel="top" title="Skyline 1.0.6-dev documentation" href="index.html"/>
        <link rel="next" title="Mirage" href="mirage.html"/>
        <link rel="prev" title="Analyzer" href="analyzer.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> Skyline
          

          
          </a>

          
            
            
              <div class="version">
                1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="requirements.html">Requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="running-in-python-virtualenv.html">Running Skyline in a Python virtualenv</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="upgrading.html">Upgrading</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-data-into-skyline.html">Getting data into Skyline</a></li>
<li class="toctree-l1"><a class="reference internal" href="alert-testing.html">Alert testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="horizon.html">Horizon</a></li>
<li class="toctree-l1"><a class="reference internal" href="analyzer.html">Analyzer</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Analyzer Optimizations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#number-of-analyzer-processors">Number of Analyzer processors</a></li>
<li class="toctree-l2"><a class="reference internal" href="#analyzer-work-rate">Analyzer work rate</a></li>
<li class="toctree-l2"><a class="reference internal" href="#performance-tuning">Performance tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-anomaly-breakdown-metrics-graphs-to-tune-the-analyzer-workflow">Using anomaly_breakdown metrics graphs to tune the Analyzer workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="#algorithm-benchmarks">algorithm benchmarks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#considerations-approximation-of-timings">Considerations - approximation of timings</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tmpfs-vs-multiprocessing-value">tmpfs vs multiprocessing Value</a></li>
<li class="toctree-l3"><a class="reference internal" href="#algorithms-ranked-by-triggered-count">Algorithms ranked by triggered count</a></li>
<li class="toctree-l3"><a class="reference internal" href="#algorithms-ranked-by-execution-time">Algorithms ranked by execution time</a></li>
<li class="toctree-l3"><a class="reference internal" href="#performance-weighting">Performance weighting</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#settings-run-optimized-workflow"><code class="docutils literal"><span class="pre">settings.RUN_OPTIMIZED_WORKFLOW</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#optimizations-results">Optimizations results</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="mirage.html">Mirage</a></li>
<li class="toctree-l1"><a class="reference internal" href="boundary.html">Boundary</a></li>
<li class="toctree-l1"><a class="reference internal" href="crucible.html">Crucible</a></li>
<li class="toctree-l1"><a class="reference internal" href="panorama.html">Panorama</a></li>
<li class="toctree-l1"><a class="reference internal" href="webapp.html">Webapp</a></li>
<li class="toctree-l1"><a class="reference internal" href="redis-integration.html">Redis integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="skyline-and-friends.html">Skyline and friends</a></li>
<li class="toctree-l1"><a class="reference internal" href="logging.html">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="tuning-tips.html">Tuning tips</a></li>
<li class="toctree-l1"><a class="reference internal" href="tuning-tips.html#smaller-deployments">Smaller deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="tuning-tips.html#reliability">Reliability</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitoring-skyline.html">Monitoring Skyline</a></li>
<li class="toctree-l1"><a class="reference internal" href="debian-and-vagrant-installation-tips.html">Debian and Vagrant Installation Tips</a></li>
<li class="toctree-l1"><a class="reference internal" href="building-documentation.html">Building documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="releases.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="whats-new.html">What&#8217;s new</a></li>
<li class="toctree-l1"><a class="reference internal" href="development/index.html">Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="roadmap.html">Roadmap</a></li>
<li class="toctree-l1"><a class="reference internal" href="skyline.html">skyline package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">Skyline</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>Analyzer Optimizations</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/analyzer-optimizations.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="analyzer-optimizations">
<h1>Analyzer Optimizations<a class="headerlink" href="#analyzer-optimizations" title="Permalink to this headline">¶</a></h1>
<p>The original implementation of Skyline has worked for years, almost
flawlessly it must be said. However the original implementation of
Skyline was patterned and somewhat even hard coded to run in a very
large and powerful setup in terms of the volume of metrics it was
handling and the server specs it was running on.</p>
<p>This setup worked and ran OK on all metric volumes. However if Skyline
was setup in a smaller environment with a few 1000 metrics, the CPU
graphs and load_avg stats of your Skyline server suggested that Skyline
did need a LOT of processing power. However this is no longer true.</p>
<p>This was due to one single line of code at the end of Analyzer module,
which only slept if the runtime of an analysis was less than 5 seconds,
undoubtedly resulted in a lot of Skyline implementations seeing
constantly high CPU usage and load_avg when running Skyline. As this
resulted in Analyzer running in say 19 seconds and then immediately
spawning again and again, etc.</p>
<p><a class="reference external" href="https://github.com/etsy/skyline/blob/master/src/analyzer/analyzer.py#L242">https://github.com/etsy/skyline/blob/master/src/analyzer/analyzer.py#L242</a></p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># Sleep if it went too fast</span>
<span class="k">if</span> <span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">now</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">:</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;sleeping due to low run time...&#39;</span><span class="p">)</span>
    <span class="n">sleep</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="number-of-analyzer-processors">
<h2>Number of Analyzer processors<a class="headerlink" href="#number-of-analyzer-processors" title="Permalink to this headline">¶</a></h2>
<p>A number of optimizations have changed the required number of processors
to assign to <a class="reference internal" href="skyline.html#settings.ANALYZER_PROCESSES" title="settings.ANALYZER_PROCESSES"><code class="xref py py-mod docutils literal"><span class="pre">settings.ANALYZER_PROCESSES</span></code></a>, quite dramatically in some
cases.  Specifically in cases where the number of metrics being analyzed is not
in the 10s of 1000s.</p>
<p>Python multiprocessing is not very efficient if it is not need, in fact
the overall overhead of the spawned processes ends up greater than the
overhead of processing with a single process. For example, if we have a
few 1000 metrics and we have 4 processors assigned to Analyzer and the
process duration is 19 seconds, in the original Analyzer we would have
seen 4 CPUs running at 100% constantly (due to the above Sleep if it
went to fast). Even if we leave to Sleep if went too fast in and we
change to <a class="reference internal" href="skyline.html#settings.ANALYZER_PROCESSES" title="settings.ANALYZER_PROCESSES"><code class="xref py py-mod docutils literal"><span class="pre">settings.ANALYZER_PROCESSES</span></code></a> to 1, we will find that:</p>
<ul class="simple">
<li><ol class="first loweralpha">
<li>we now see 1 CPU running at 100%</li>
</ol>
</li>
<li><ol class="first loweralpha" start="2">
<li>our duration has probably increased to about 27 seconds</li>
</ol>
</li>
<li><ol class="first loweralpha" start="3">
<li>we use a little more memory on the single process</li>
</ol>
</li>
</ul>
<p>When we optimize the sleep to match the environment with the
<a class="reference internal" href="skyline.html#settings.ANALYZER_OPTIMUM_RUN_DURATION" title="settings.ANALYZER_OPTIMUM_RUN_DURATION"><code class="xref py py-mod docutils literal"><span class="pre">settings.ANALYZER_OPTIMUM_RUN_DURATION</span></code></a> of in this case say 60 instead of
5, we will find that:</p>
<ul class="simple">
<li><ol class="first loweralpha">
<li>we now see 1 CPU running at a few percent, only spiking up for 27 seconds</li>
</ol>
</li>
</ul>
<p>When we further optimize and use the <a class="reference internal" href="skyline.html#settings.RUN_OPTIMIZED_WORKFLOW" title="settings.RUN_OPTIMIZED_WORKFLOW"><code class="xref py py-mod docutils literal"><span class="pre">settings.RUN_OPTIMIZED_WORKFLOW</span></code></a> we
will find that:</p>
<ul class="simple">
<li><ol class="first loweralpha">
<li>we now see 1 CPU running at a few percent, only spiking up for 14 seconds</li>
</ol>
</li>
<li><ol class="first loweralpha" start="2">
<li>our duration has probably decreased to about 50%</li>
</ol>
</li>
</ul>
<p>See <a class="reference internal" href="#optimizations-results">Optimizations results</a> at the end of this page.</p>
</div>
<div class="section" id="analyzer-work-rate">
<h2>Analyzer work rate<a class="headerlink" href="#analyzer-work-rate" title="Permalink to this headline">¶</a></h2>
<p>The original Analyzer analyzed all timeseries against all algorithms
which is the maximum possible work. In terms of the <a class="reference internal" href="skyline.html#settings.CONSENSUS" title="settings.CONSENSUS"><code class="xref py py-mod docutils literal"><span class="pre">settings.CONSENSUS</span></code></a>
based model, this is not the most efficient work rate.</p>
</div>
<div class="section" id="performance-tuning">
<h2>Performance tuning<a class="headerlink" href="#performance-tuning" title="Permalink to this headline">¶</a></h2>
<p>A special thanks to <a class="reference external" href="https://jiffyclub.github.io/snakeviz/">Snakeviz</a>
for a very useful Python profiling tool which enabled some minimal
changes in the code and substantial improvements in the performance,
along with <a class="reference external" href="https://github.com/ymichael/cprofilev">cprofilev</a> and
<a class="reference external" href="https://github.com/vmprof/vmprof-python">vmprof</a>.</p>
</div>
<div class="section" id="using-anomaly-breakdown-metrics-graphs-to-tune-the-analyzer-workflow">
<h2>Using anomaly_breakdown metrics graphs to tune the Analyzer workflow<a class="headerlink" href="#using-anomaly-breakdown-metrics-graphs-to-tune-the-analyzer-workflow" title="Permalink to this headline">¶</a></h2>
<p>anomaly_breakdown metrics were added to Skyline on 10 Jun 2014, yet
never merged into the main Etsy fork. However, in terms of performance
tuning and profiling Skyline they are quite useful. They provide us with
the ability to optimize the analysis of timeseries data based on 2
simple criteria:</p>
<ol class="arabic simple">
<li>Determine the algorithms that are triggered most frequently</li>
<li>Determine the computational expense of each algorithm (a development
addition) that adds <code class="docutils literal"><span class="pre">algorithm_breakdown.*.timing.times_run</span></code> and
the <code class="docutils literal"><span class="pre">algorithm_breakdown.*.timing.total_time</span></code> metrics to
skyline.analyzer.  <code class="docutils literal"><span class="pre">hostname</span></code> graphite metric namespaces.</li>
</ol>
<p>We can use these data to determine the <em>efficiency</em> of the algorithms
and when this is applied to the Analyzer <a class="reference internal" href="skyline.html#settings.CONSENSUS" title="settings.CONSENSUS"><code class="xref py py-mod docutils literal"><span class="pre">settings.CONSENSUS</span></code></a> model we can
optimize <code class="docutils literal"><span class="pre">algorithms.py</span></code> to run in the most efficient manner possible.</p>
<p>Originally algorithms.py simply analyzed every timeseries against every
<code class="docutils literal"><span class="pre">algorithm</span> <span class="pre">in</span> <span class="pre">ALGORITHMS</span></code> and only checked the <a class="reference internal" href="skyline.html#settings.CONSENSUS" title="settings.CONSENSUS"><code class="xref py py-mod docutils literal"><span class="pre">settings.CONSENSUS</span></code></a>
threshold at the end. However a very small but effective optimization is to use
the above data to run the following optimizations.</p>
<ul class="simple">
<li>The most frequently and least expensive <a class="reference internal" href="skyline.html#settings.CONSENSUS" title="settings.CONSENSUS"><code class="xref py py-mod docutils literal"><span class="pre">settings.CONSENSUS</span></code></a> number of
algorithms and then determine if <a class="reference internal" href="skyline.html#settings.CONSENSUS" title="settings.CONSENSUS"><code class="xref py py-mod docutils literal"><span class="pre">settings.CONSENSUS</span></code></a> can be achieved.
Currently there are 9 algorithms that Analyzer uses. However the same
optimization is valid if more algorithms were added.</li>
<li>If our <a class="reference internal" href="skyline.html#settings.CONSENSUS" title="settings.CONSENSUS"><code class="xref py py-mod docutils literal"><span class="pre">settings.CONSENSUS</span></code></a> was 6 and Analyzer has not been able to
trigger any of the most frequently and least expensive 5 algorithms, then
there is no need to analyze the timeseries against the remaining
algorithms. This surprisingly reduces the work of Analyzer by ~xx% on average
(a lot).</li>
<li>The cost of this optimization is that we lose the original
<code class="docutils literal"><span class="pre">algorithm_breakdown.*</span></code> metrics which this was evaluated and patterned
against. However two additional factors somewhat mitigate this but it is
definitely still skewed. The mitigations being that:<ul>
<li>When a timeseries is anomalous more than one algorithm triggers
anyway.</li>
<li>When an algorithm is triggered, more algorithms are run. Seeing as we
have optimized to have the least frequently triggered algorithms be
run later in the workflow, it stands to reason that a lot of the
time, they would not have triggered even if they were run. However it
is still skewed.</li>
</ul>
</li>
</ul>
<p>These optimizations are now the default in <code class="docutils literal"><span class="pre">settings.py</span></code>, however they
have been implemented with backwards compatibility and for the purpose
of running Analyzer without optimization of the algorithms to ensure
that they can be benchmarked again should any further algorithms ever be
added to Analyzer or any existing algorithms modified in any way.</p>
</div>
<div class="section" id="algorithm-benchmarks">
<h2>algorithm benchmarks<a class="headerlink" href="#algorithm-benchmarks" title="Permalink to this headline">¶</a></h2>
<p>analyzer_dev can be/was used as a benchmarking module to determine the
execution times of algorithms.</p>
<div class="section" id="considerations-approximation-of-timings">
<h3>Considerations - approximation of timings<a class="headerlink" href="#considerations-approximation-of-timings" title="Permalink to this headline">¶</a></h3>
<p>The algorithm benchmark timings are simply approximations of the real
times that the algorithm execution is undertaken in (float).</p>
</div>
<div class="section" id="tmpfs-vs-multiprocessing-value">
<h3>tmpfs vs multiprocessing Value<a class="headerlink" href="#tmpfs-vs-multiprocessing-value" title="Permalink to this headline">¶</a></h3>
<p>Recording the algorithm counts and timings without using multiprocessing Value
and associated overhead of locks, etc, etc. /tmp was opted for instead and the
variable <a class="reference internal" href="skyline.html#settings.SKYLINE_TMP_DIR" title="settings.SKYLINE_TMP_DIR"><code class="xref py py-mod docutils literal"><span class="pre">settings.SKYLINE_TMP_DIR</span></code></a> was added. In most cases /tmp is tmpfs
which is memory anyway so all the heavy lifting in terms of locking etc is
offloaded to the OS and modules do not have to incur the additional complexity
in Python. A simple yet effective win. Same same but different. There may be
some valid reasons for the use multiprocessing Value or Manager().list()</p>
</div>
<div class="section" id="algorithms-ranked-by-triggered-count">
<h3>Algorithms ranked by triggered count<a class="headerlink" href="#algorithms-ranked-by-triggered-count" title="Permalink to this headline">¶</a></h3>
<p>Using the <code class="docutils literal"><span class="pre">anomaly_breakdown</span></code> metrics data it shows that on a plethora of
machine and application related metrics, we can determine the most
triggered algorithms by rank:</p>
<ol class="arabic simple">
<li>stddev_from_average</li>
<li>mean_subtraction_cumulation</li>
<li>first_hour_average</li>
<li>histogram_bins</li>
<li>least_squares</li>
<li>grubbs</li>
<li>stddev_from_moving_average</li>
<li>median_absolute_deviation</li>
<li>ks_test</li>
</ol>
</div>
<div class="section" id="algorithms-ranked-by-execution-time">
<h3>Algorithms ranked by execution time<a class="headerlink" href="#algorithms-ranked-by-execution-time" title="Permalink to this headline">¶</a></h3>
<p>Using the <code class="docutils literal"><span class="pre">algorithm_breakdown</span></code> metrics data we can determine the most
&#8220;expensive&#8221; algorithms by total time to run:</p>
<ol class="arabic simple">
<li>least_squares (avg: 0.563052576667)</li>
<li>stddev_from_moving_average (avg: 0.48511087)</li>
<li>mean_subtraction_cumulation (avg: 0.453279348333)</li>
<li>median_absolute_deviation (avg: 0.25222528)</li>
<li>stddev_from_average (avg: 0.173473198333)</li>
<li>first_hour_average (avg: 0.151071298333)</li>
<li>grubbs (avg: 0.147807641667)</li>
<li>histogram_bins (avg: 0.101075738333)</li>
<li>ks_test (avg: 0.0979568116667)</li>
</ol>
</div>
<div class="section" id="performance-weighting">
<h3>Performance weighting<a class="headerlink" href="#performance-weighting" title="Permalink to this headline">¶</a></h3>
<p>If we change the order in which the timeseries are run through the
algorithms in Analyzer, we can improve the overall performance by
running the most expensive computational algorithms later in the
analysis.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">+-----------------------------+----------------+---------------------+</span>
<span class="o">|</span> <span class="n">Algorithm</span>                   <span class="o">|</span> <span class="n">Triggered</span> <span class="n">rank</span> <span class="o">|</span> <span class="n">Execution</span> <span class="n">time</span> <span class="n">rank</span> <span class="o">|</span>
<span class="o">+=============================+================+=====================+</span>
<span class="o">|</span> <span class="n">histogram_bins</span>              <span class="o">|</span> <span class="mi">4</span>              <span class="o">|</span> <span class="mi">8</span>                   <span class="o">|</span>
<span class="o">+-----------------------------+----------------+---------------------+</span>
<span class="o">|</span> <span class="n">first_hour_average</span>          <span class="o">|</span> <span class="mi">3</span>              <span class="o">|</span> <span class="mi">6</span>                   <span class="o">|</span>
<span class="o">+-----------------------------+----------------+---------------------+</span>
<span class="o">|</span> <span class="n">stddev_from_average</span>         <span class="o">|</span> <span class="mi">1</span>              <span class="o">|</span> <span class="mi">5</span>                   <span class="o">|</span>
<span class="o">+-----------------------------+----------------+---------------------+</span>
<span class="o">|</span> <span class="n">grubbs</span>                      <span class="o">|</span> <span class="mi">6</span>              <span class="o">|</span> <span class="mi">7</span>                   <span class="o">|</span>
<span class="o">+-----------------------------+----------------+---------------------+</span>
<span class="o">|</span> <span class="n">ks_test</span>                     <span class="o">|</span> <span class="mi">9</span>              <span class="o">|</span> <span class="mi">9</span>                   <span class="o">|</span>
<span class="o">+-----------------------------+----------------+---------------------+</span>
<span class="o">|</span> <span class="n">mean_subtraction_cumulation</span> <span class="o">|</span> <span class="mi">3</span>              <span class="o">|</span> <span class="mi">2</span>                   <span class="o">|</span>
<span class="o">+-----------------------------+----------------+---------------------+</span>
<span class="o">|</span> <span class="n">median_absolute_deviation</span>   <span class="o">|</span> <span class="mi">8</span>              <span class="o">|</span> <span class="mi">4</span>                   <span class="o">|</span>
<span class="o">+-----------------------------+----------------+---------------------+</span>
<span class="o">|</span> <span class="n">stddev_from_moving_average</span>  <span class="o">|</span> <span class="mi">7</span>              <span class="o">|</span> <span class="mi">2</span>                   <span class="o">|</span>
<span class="o">+-----------------------------+----------------+---------------------+</span>
<span class="o">|</span> <span class="n">least_squares</span>               <span class="o">|</span> <span class="mi">5</span>              <span class="o">|</span> <span class="mi">1</span>                   <span class="o">|</span>
<span class="o">+-----------------------------+----------------+---------------------+</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="settings-run-optimized-workflow">
<h2><a class="reference internal" href="skyline.html#settings.RUN_OPTIMIZED_WORKFLOW" title="settings.RUN_OPTIMIZED_WORKFLOW"><code class="xref py py-mod docutils literal"><span class="pre">settings.RUN_OPTIMIZED_WORKFLOW</span></code></a><a class="headerlink" href="#settings-run-optimized-workflow" title="Permalink to this headline">¶</a></h2>
<p>The original version of Analyzer ran all timeseries through all
<code class="docutils literal"><span class="pre">ALGORITHMS</span></code> like so:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">ensemble</span> <span class="o">=</span> <span class="p">[</span><span class="nb">globals</span><span class="p">()[</span><span class="n">algorithm</span><span class="p">](</span><span class="n">timeseries</span><span class="p">)</span> <span class="k">for</span> <span class="n">algorithm</span> <span class="ow">in</span> <span class="n">ALGORITHMS</span><span class="p">]</span>
</pre></div>
</div>
<p>After running all the algorithms, it then determined whether the last
datapoint for timeseries was anomalous.</p>
<p>The optimized workflow uses the above triggered / execution time ranking
matrix to run as efficiently as possible and achieve the same results
(see caveat below) but up to ~50% quicker and less CPU cycles. This is
done by iterating through the algorithms in order based on their
respective matrix rankings and evaluating the whether <a class="reference internal" href="skyline.html#settings.CONSENSUS" title="settings.CONSENSUS"><code class="xref py py-mod docutils literal"><span class="pre">settings.CONSENSUS</span></code></a>
can be achieved or not. The least_squares algorithm, which is the most
computationally expensive, now only runs if <a class="reference internal" href="skyline.html#settings.CONSENSUS" title="settings.CONSENSUS"><code class="xref py py-mod docutils literal"><span class="pre">settings.CONSENSUS</span></code></a> can be
achieved.</p>
<p>The caveat to this is that this skews the <code class="docutils literal"><span class="pre">anomaly_breakdown</span></code> metrics.
However seeing as the <code class="docutils literal"><span class="pre">anomaly_breakdown</span></code> metrics were not part of the
original Analyzer this is a mute point. That said the performance tuning
and optimizations were made possible by these data, therefore it remains
possible to implement the original configuration and also time all
algorithms (see Development modes if you are interested). A word of
warning, if you have setup a Skyline implementation after the
<a class="reference internal" href="skyline.html#settings.RUN_OPTIMIZED_WORKFLOW" title="settings.RUN_OPTIMIZED_WORKFLOW"><code class="xref py py-mod docutils literal"><span class="pre">settings.RUN_OPTIMIZED_WORKFLOW</span></code></a> and you have &gt; 1000 metrics running the
unoptimized workflow with the original 5 seconds may send the load_avg
through the roof.</p>
<p>The original Analyzer <a class="reference internal" href="skyline.html#settings.ALGORITHMS" title="settings.ALGORITHMS"><code class="xref py py-mod docutils literal"><span class="pre">settings.ALGORITHMS</span></code></a> setting was:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">ALGORITHMS</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;first_hour_average&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mean_subtraction_cumulation&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stddev_from_average&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stddev_from_moving_average&#39;</span><span class="p">,</span>
    <span class="s1">&#39;least_squares&#39;</span><span class="p">,</span>
    <span class="s1">&#39;grubbs&#39;</span><span class="p">,</span>
    <span class="s1">&#39;histogram_bins&#39;</span><span class="p">,</span>
    <span class="s1">&#39;median_absolute_deviation&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ks_test&#39;</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
<p>The new optimized Analyzer <a class="reference internal" href="skyline.html#settings.ALGORITHMS" title="settings.ALGORITHMS"><code class="xref py py-mod docutils literal"><span class="pre">settings.ALGORITHMS</span></code></a> setting based on the above
performance weighing matrix is:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">ALGORITHMS</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;histogram_bins&#39;</span><span class="p">,</span>
    <span class="s1">&#39;first_hour_average&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stddev_from_average&#39;</span><span class="p">,</span>
    <span class="s1">&#39;grubbs&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ks_test&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mean_subtraction_cumulation&#39;</span><span class="p">,</span>
    <span class="s1">&#39;median_absolute_deviation&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stddev_from_moving_average&#39;</span><span class="p">,</span>
    <span class="s1">&#39;least_squares&#39;</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="optimizations-results">
<h2>Optimizations results<a class="headerlink" href="#optimizations-results" title="Permalink to this headline">¶</a></h2>
<p>These server graphs show the pre and post crucible update metrics related to CPU
and loadavg for a dedicated Skyline server running on a 4 vCPU, 4GB RAM, SSD
cloud server.  The server is handling ~3000 metrics and is solely dedicated to
running Skyline.  It was upgraded from the Boundary branch version to Crucible.</p>
<p>It was running Analyzer, Mirage and Boundary, however these graphs clearly show
the impact that the Analyzer optimizations have on the overall workload.
Interestingly, after deployment the server is also running MySQL and the
Panorama daemon in addition to what it was running before.</p>
<p>The server was running the skyline.analyzer.metrics branch Analyzer which was
only a few steps away from Etsy master and those steps were related to Analyzer
sending a few more Skyline metric namespaces to Graphite, in terms of Analyzer
and the algorithms logic pre update were identical to Etsy master.</p>
<div class="figure">
<img alt="_images/analyzer.skyline1.runtime.png" src="_images/analyzer.skyline1.runtime.png" />
</div>
<div class="figure">
<img alt="_images/analyzer.optimizations.cpu0.png" src="_images/analyzer.optimizations.cpu0.png" />
</div>
<div class="figure">
<img alt="_images/analyzer.optimizations.cpu1.png" src="_images/analyzer.optimizations.cpu1.png" />
</div>
<div class="figure">
<img alt="_images/analyzer.optimizations.cpu2.png" src="_images/analyzer.optimizations.cpu2.png" />
</div>
<div class="figure">
<img alt="_images/analyzer.optimizations.cpu3.png" src="_images/analyzer.optimizations.cpu3.png" />
</div>
<div class="figure">
<img alt="_images/analyzer.optimizations.loadavg.png" src="_images/analyzer.optimizations.loadavg.png" />
</div>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="mirage.html" class="btn btn-neutral float-right" title="Mirage" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="analyzer.html" class="btn btn-neutral" title="Analyzer" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2013-2014, Etsy Inc; 2015, Abe Stanway; 2015-2016, Gary Wilson.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'1.0.6-dev',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>